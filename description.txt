Есть csv файл, расположенный по ссылке. Скачивайте. Размер распакованного файла около 300 Мб. Количество строк - 590708 строк.

Данные в файле:
ID объекта (например, 590707)
Координаты (Широта, Долгота) (например, 55.060199, 32.695577)
Год постройки (например, 1953.0)
Площадь (например, 585.60)
Количество этажей (например, 18)
Область и город (например, Смоленская область, Ярцево)
Адрес (например, "ул. Братьев Шаршановых, д. 61")
Описание объекта (например, "Жилой дом в Ярцево, по адресу ул. Братьев Шаршановых, д. 61, 1953 года постройки, под управлением ТСЖ «Шаршановых».")
Что необходимо сделать?

-1. Обратите внимание, что в проекте используются PySpark и Clickhouse. PySpark в airflow в docker-compose нету! Его необходимо добавить самостоятельно. Куда вставить в docker-compose - задача самостоятельная. 

0. Предварительно разверните docker-compose из первых степов! Шаги 1-7 должны быть сделаны в DAG.

1. Загрузите файл данных в DataFrame PySpark. Обязательно выведите количество строк.

2. Убедитесь, что данные корректно прочитаны (правильный формат, отсутствие пустых строк).

3. Преобразуйте текстовые и числовые поля в соответствующие типы данных (например, дата, число).

4. Вычислите средний и медианный год постройки зданий.

5. Определите топ-10 областей и городов с наибольшим количеством объектов.

6. Найдите здания с максимальной и минимальной площадью в рамках каждой области.

7. Определите количество зданий по десятилетиям (например, сколько зданий построено в 1950-х, 1960-х и т.д.).

С данными поработали, проанализировали, теперь можно их и в БД положить. 

8. Создайте схему таблицы в ClickHouse, которая будет соответствовать структуре ваших данных. Это можно сделать не через airflow.

9. Настройте соединение с ClickHouse из скрипта, учтите, что сделать это необходимо в airflow.

10. Загрузите обработанные данные из DataFrame в таблицу в ClickHouse (airflow)

11. Выполните SQL скрипт в Python, который выведет топ 25 домов, у которых площадь больше 60 кв.м (airflow)

В качестве решения необходимо прислать ссылку на репозиторий. Обратите внимание на критерии (их 4) сдачи проекта - 

1. Оформление. Репозиторий оформлен так, как нужно. А именно есть структура, удобное содержание, читаемый README, естественно наличие условия задания. Обязательно наличие docker-compose. Обязательно наличие скрипта py, код в котором обернут в DAG.

2. Логика. Все сделано в одном скрипте. Сначала читаем данные с csv, потом преобразуем данные, анализируем, загружаем в кликхаус, читаем с кликхауса. Все при этом находится в Airflow. Откуда загружать csv - думаем или гуглим (а лучше оба сразу действия делать)

3. Содержание. Файл c csv хранить в GIT нельзя! Добавляйте либо ссылку, либо срез данных.

4. Корректность. Данные нельзя редактировать, файл нельзя обрезать. Результаты аналитики и SQL скрипта должны быть правильными.

Дополнительно. Можно сделать графики на MatPlotLib для 5, 6 и 7 пунктов.