# Инструкция по работе с датасетом russian_houses.csv

## О датасете

- **Размер файла**: ~300 МБ (распакованный)
- **Количество строк**: 590,708 строк
- **Формат**: CSV (UTF-8)

## Структура данных

Файл содержит следующие колонки (без заголовков):

1. **ID объекта** (integer) - Уникальный идентификатор, например: 590707
2. **Широта** (float) - Географическая широта, например: 55.060199
3. **Долгота** (float) - Географическая долгота, например: 32.695577
4. **Год постройки** (integer/float) - Год постройки здания, например: 1953.0
5. **Площадь** (float) - Площадь здания в кв.м, например: 585.60
6. **Количество этажей** (integer) - Этажность здания, например: 18
7. **Область** (string) - Регион/область, например: "Смоленская область"
8. **Город** (string) - Город, например: "Ярцево"
9. **Адрес** (string) - Адрес здания, например: "ул. Братьев Шаршановых, д. 61"
10. **Описание** (string) - Полное описание объекта

## Пример данных

См. файл `russian_houses_sample.csv` с образцами данных.

## Важно для Git

⚠️ **НЕ ХРАНИТЕ** файл `russian_houses.csv` в Git репозитории!

- Файл слишком большой для Git (~300 МБ)
- Файл добавлен в `.gitignore`
- Используйте внешнее хранилище или ссылку для скачивания

## Размещение файла

Для работы проекта файл `russian_houses.csv` должен находиться в корневой директории:

```
nova_data_task_3/
├── russian_houses.csv          ← Здесь должен быть файл!
├── docker-compose.yml
├── Dockerfile
└── ...
```

## Автоматическая загрузка в контейнер

Docker Compose автоматически смонтирует этот файл в контейнер Airflow по пути:
```
/opt/airflow/russian_houses.csv
```

Именно этот путь используется в DAG для чтения данных.

## Проверка наличия файла

### В Windows PowerShell:
```powershell
Test-Path .\russian_houses.csv
```

### В Docker контейнере:
```bash
docker-compose exec airflow-webserver ls -lh /opt/airflow/russian_houses.csv
```

## Альтернативные способы получения файла

Если файл отсутствует, вы можете:

1. **Скачать из облачного хранилища** (Google Drive, Яндекс.Диск, и т.д.)
2. **Использовать wget/curl в DAG** для автоматической загрузки
3. **Загрузить вручную** и поместить в корневую директорию

### Пример автоматической загрузки в DAG:

```python
import urllib.request

def download_csv():
    url = "https://your-storage.com/russian_houses.csv"
    local_path = "/opt/airflow/russian_houses.csv"
    urllib.request.urlretrieve(url, local_path)
```

## Обработка больших файлов

DAG использует PySpark для эффективной обработки больших объемов данных:

- Распределенная обработка
- Ленивые вычисления
- Оптимизация памяти
- Батч-загрузка в ClickHouse

## Форматы хранения

После обработки данные сохраняются:
- **ClickHouse** - для быстрых аналитических запросов
- **Графики PNG** - визуализация результатов в `/data`
