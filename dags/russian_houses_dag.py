"""
DAG –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –¥–∞–Ω–Ω—ã—Ö –æ —Ä–æ—Å—Å–∏–π—Å–∫–∏—Ö –¥–æ–º–∞—Ö —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º PySpark –∏ ClickHouse
"""
from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.python import PythonOperator
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, year, floor, count, max as spark_max, min as spark_min, avg, expr
from pyspark.sql.types import DoubleType, IntegerType
import clickhouse_connect
import os
import matplotlib
matplotlib.use('Agg')  # Use non-interactive backend
import matplotlib.pyplot as plt

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è ClickHouse –∏–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è
CH_HOST = os.getenv('CH_HOST', 'clickhouse')
CH_PORT = int(os.getenv('CH_PORT', 8123))
CH_USER = os.getenv('CH_USER', 'default')
CH_PASSWORD = os.getenv('CH_PASSWORD', 'clickhouse123')
CH_DATABASE = os.getenv('CH_DATABASE', 'russian_houses_db')

# –ü—É—Ç–∏ –∫ —Ñ–∞–π–ª–∞–º
CSV_FILE_PATH = '/opt/airflow/data/russian_houses.csv'
PARQUET_FILE_PATH = '/opt/airflow/data/russian_houses.parquet'

# –ì–ª–æ–±–∞–ª—å–Ω–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è –¥–ª—è SparkSession
spark = None


def get_spark_session():
    """–°–æ–∑–¥–∞–Ω–∏–µ –∏–ª–∏ –ø–æ–ª—É—á–µ–Ω–∏–µ SparkSession"""
    global spark
    if spark is None:
        spark = SparkSession.builder \
            .appName("RussianHousesAnalysis") \
            .master("local[*]") \
            .config("spark.driver.memory", "4g") \
            .config("spark.executor.memory", "4g") \
            .config("spark.sql.shuffle.partitions", "8") \
            .getOrCreate()
    return spark


def read_parquet_data(spark):
    """–ß—Ç–µ–Ω–∏–µ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏–∑ Parquet"""
    df = spark.read.parquet(PARQUET_FILE_PATH)
    print(f"‚úÖ Parquet –ø—Ä–æ—á–∏—Ç–∞–Ω. –ö–æ–ª–æ–Ω–∫–∏: {df.columns}")
    return df


def read_csv_with_bom_fix(spark):
    """–ß—Ç–µ–Ω–∏–µ CSV —Å –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π –∫–æ–¥–∏—Ä–æ–≤–∫–æ–π (UTF-16 LE) –∏ –ø–æ–ª–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏"""
    # CSV —Ñ–∞–π–ª –∏–º–µ–µ—Ç UTF-16 LE BOM (0xFF 0xFE), –ø–æ—ç—Ç–æ–º—É —á–∏—Ç–∞–µ–º —Å –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π –∫–æ–¥–∏—Ä–æ–≤–∫–æ–π
    df = spark.read.csv(
        CSV_FILE_PATH,
        header=True,
        encoding='UTF-16LE',
        quote='"',
        escape='"',
        multiLine=True,
        mode="PERMISSIVE",
        columnNameOfCorruptRecord="_corrupt_record"
    )
    
    print(f"‚úÖ CSV –ø—Ä–æ—á–∏—Ç–∞–Ω. –ö–æ–ª–æ–Ω–∫–∏: {df.columns}")
    
    return df


def stop_spark_session():
    """–û—Å—Ç–∞–Ω–æ–≤–∫–∞ SparkSession"""
    global spark
    if spark is not None:
        spark.stop()
        spark = None


def prepare_data_to_parquet(**context):
    """
    –ó–∞–¥–∞—á–∞ 0: –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö - —á—Ç–µ–Ω–∏–µ CSV –æ–¥–∏–Ω —Ä–∞–∑, –æ—á–∏—Å—Ç–∫–∞ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ Parquet
    –≠—Ç–æ –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç, —á—Ç–æ –≤—Å–µ –ø–æ—Å–ª–µ–¥—É—é—â–∏–µ –∑–∞–¥–∞—á–∏ —Ä–∞–±–æ—Ç–∞—é—Ç —Å –æ–¥–Ω–∏–º–∏ –∏ —Ç–µ–º–∏ –∂–µ –æ—á–∏—â–µ–Ω–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
    """
    print("=" * 80)
    print("–ó–ê–î–ê–ß–ê 0: –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö - –ß—Ç–µ–Ω–∏–µ CSV –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ Parquet")
    print("=" * 80)
    
    spark = get_spark_session()
    
    # –ß–∏—Ç–∞–µ–º CSV —Å –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π –∫–æ–¥–∏—Ä–æ–≤–∫–æ–π
    df = read_csv_with_bom_fix(spark)
    
    # –ü–æ–¥—Å—á–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Å—Ç—Ä–æ–∫
    row_count = df.count()
    print(f"\n‚úÖ –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–æ–∫ –≤ –∏—Å—Ö–æ–¥–Ω–æ–º CSV: {row_count}")
    
    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Å—Ö–µ–º—É –¥–∞–Ω–Ω—ã—Ö
    print("\n–ò—Å—Ö–æ–¥–Ω–∞—è —Å—Ö–µ–º–∞ –¥–∞–Ω–Ω—ã—Ö:")
    df.printSchema()
    
    # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —Ç–∏–ø–æ–≤ –¥–∞–Ω–Ω—ã—Ö
    print("\n–ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —Ç–∏–ø–æ–≤ –¥–∞–Ω–Ω—ã—Ö...")
    df_transformed = df \
        .withColumn("house_id", col("house_id").cast(IntegerType())) \
        .withColumn("latitude", col("latitude").cast(DoubleType())) \
        .withColumn("longitude", col("longitude").cast(DoubleType())) \
        .withColumn("maintenance_year", col("maintenance_year").cast(IntegerType())) \
        .withColumn("square", col("square").cast(DoubleType())) \
        .withColumn("population", col("population").cast(IntegerType())) \
        .withColumnRenamed("house_id", "id") \
        .withColumnRenamed("maintenance_year", "year") \
        .withColumnRenamed("square", "area") \
        .withColumnRenamed("population", "floors") \
        .withColumnRenamed("locality_name", "city")
    
    # –£–¥–∞–ª—è–µ–º —Å—Ç—Ä–æ–∫–∏ —Å null –∑–Ω–∞—á–µ–Ω–∏—è–º–∏ –≤ –∫—Ä–∏—Ç–∏—á–Ω—ã—Ö –ø–æ–ª—è—Ö
    df_clean = df_transformed.filter(
        col("id").isNotNull() & 
        col("year").isNotNull() & 
        col("area").isNotNull()
    ).select("id", "latitude", "longitude", "year", "area", "floors", "region", "city", "address", "description")
    
    clean_count = df_clean.count()
    print(f"‚úÖ –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–æ–∫ –ø–æ—Å–ª–µ –æ—á–∏—Å—Ç–∫–∏: {clean_count}")
    
    print("\n–ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–Ω–∞—è —Å—Ö–µ–º–∞ –¥–∞–Ω–Ω—ã—Ö:")
    df_clean.printSchema()
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—á–∏—â–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –≤ Parquet (–ø–µ—Ä–µ–∑–∞–ø–∏—Å—ã–≤–∞–µ–º, –µ—Å–ª–∏ —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç)
    print(f"\n–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ—á–∏—â–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –≤ {PARQUET_FILE_PATH}...")
    df_clean.coalesce(1).write.mode("overwrite").parquet(PARQUET_FILE_PATH)
    print(f"‚úÖ –î–∞–Ω–Ω—ã–µ —É—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ Parquet")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –≤ XCom
    context['ti'].xcom_push(key='total_rows', value=row_count)
    context['ti'].xcom_push(key='clean_rows', value=clean_count)
    
    return clean_count


def load_csv_to_spark(**context):
    """
    –ó–∞–¥–∞—á–∞ 1: –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö (–¥–∞–Ω–Ω—ã–µ —É–∂–µ –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω—ã –≤ –∑–∞–¥–∞—á–µ 0)
    """
    print("=" * 80)
    print("–ó–ê–î–ê–ß–ê 1: –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö")
    print("=" * 80)
    
    spark = get_spark_session()
    df = read_parquet_data(spark)
    
    # –ü–æ–¥—Å—á–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Å—Ç—Ä–æ–∫
    row_count = df.count()
    print(f"\n‚úÖ –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–æ–∫ –≤ –¥–∞—Ç–∞—Å–µ—Ç–µ: {row_count}")
    
    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Å—Ö–µ–º—É –¥–∞–Ω–Ω—ã—Ö
    print("\n–°—Ö–µ–º–∞ –¥–∞–Ω–Ω—ã—Ö:")
    df.printSchema()
    
    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ –Ω–µ—Å–∫–æ–ª—å–∫–æ —Å—Ç—Ä–æ–∫
    print("\n–ü–µ—Ä–≤—ã–µ 5 —Å—Ç—Ä–æ–∫ –¥–∞–Ω–Ω—ã—Ö:")
    df.show(5, truncate=False)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–æ–∫ –≤ XCom
    context['ti'].xcom_push(key='row_count', value=row_count)
    
    return row_count


def validate_data(**context):
    """
    –ó–∞–¥–∞—á–∞ 2: –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç–∏ –¥–∞–Ω–Ω—ã—Ö (–¥–∞–Ω–Ω—ã–µ —É–∂–µ –æ—á–∏—â–µ–Ω—ã –≤ –∑–∞–¥–∞—á–µ 0)
    """
    print("=" * 80)
    print("–ó–ê–î–ê–ß–ê 2: –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç–∏ –¥–∞–Ω–Ω—ã—Ö")
    print("=" * 80)
    
    spark = get_spark_session()
    df = read_parquet_data(spark)
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏
    total_rows = df.count()
    print(f"\n–û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–æ–∫: {total_rows}")
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ null –∑–Ω–∞—á–µ–Ω–∏—è –≤ –∫–∞–∂–¥–æ–π –∫–æ–ª–æ–Ω–∫–µ
    print("\n–ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ NULL –∑–Ω–∞—á–µ–Ω–∏—è:")
    for column in df.columns:
        null_count = df.filter(col(column).isNull()).count()
        null_percentage = (null_count / total_rows) * 100 if total_rows > 0 else 0
        print(f"  {column}: {null_count} NULL ({null_percentage:.2f}%)")
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞, —á—Ç–æ –¥–∞–Ω–Ω—ã–µ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ –ø—Ä–æ—á–∏—Ç–∞–Ω—ã
    print("\n‚úÖ –î–∞–Ω–Ω—ã–µ —É—Å–ø–µ—à–Ω–æ –ø—Ä–æ—á–∏—Ç–∞–Ω—ã")
    print(f"‚úÖ –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–æ–ª–æ–Ω–æ–∫: {len(df.columns)}")
    print(f"‚úÖ –ù–∞–∑–≤–∞–Ω–∏—è –∫–æ–ª–æ–Ω–æ–∫: {df.columns}")
    
    return True


def transform_data(**context):
    """
    –ó–∞–¥–∞—á–∞ 3: –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–Ω—ã—Ö —Ç–∏–ø–æ–≤ –¥–∞–Ω–Ω—ã—Ö (–ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —É–∂–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–æ –≤ –∑–∞–¥–∞—á–µ 0)
    """
    print("=" * 80)
    print("–ó–ê–î–ê–ß–ê 3: –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–Ω—ã—Ö —Ç–∏–ø–æ–≤ –¥–∞–Ω–Ω—ã—Ö")
    print("=" * 80)
    
    spark = get_spark_session()
    df = read_parquet_data(spark)
    
    print("\n–°—Ö–µ–º–∞ –¥–∞–Ω–Ω—ã—Ö:")
    df.printSchema()
    
    print("\n–¢–∏–ø—ã –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Å—Ç–æ–ª–±—Ü–∞:")
    for column_name, data_type in df.dtypes:
        print(f"  {column_name}: {data_type}")
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Å—Ç—Ä–æ–∫ –ø–æ—Å–ª–µ –æ—á–∏—Å—Ç–∫–∏
    clean_count = df.count()
    print(f"\n‚úÖ –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–æ–∫ –ø–æ—Å–ª–µ –æ—á–∏—Å—Ç–∫–∏: {clean_count}")
    
    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–∏–º–µ—Ä—ã –¥–∞–Ω–Ω—ã—Ö
    print("\n–ü—Ä–∏–º–µ—Ä—ã –¥–∞–Ω–Ω—ã—Ö:")
    df.show(10, truncate=False)
    
    context['ti'].xcom_push(key='clean_count', value=clean_count)
    
    return clean_count


def calculate_year_statistics(**context):
    """
    –ó–∞–¥–∞—á–∞ 4: –í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Å—Ä–µ–¥–Ω–µ–≥–æ –∏ –º–µ–¥–∏–∞–Ω–Ω–æ–≥–æ –≥–æ–¥–∞ –ø–æ—Å—Ç—Ä–æ–π–∫–∏ –∑–¥–∞–Ω–∏–π
    """
    print("=" * 80)
    print("–ó–ê–î–ê–ß–ê 4: –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –≥–æ–¥–∞–º –ø–æ—Å—Ç—Ä–æ–π–∫–∏")
    print("=" * 80)
    
    spark = get_spark_session()
    df = read_parquet_data(spark)
    
    # –§–∏–ª—å—Ç—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ —Å –Ω–µ-null –∑–Ω–∞—á–µ–Ω–∏—è–º–∏ –≥–æ–¥–∞
    df_filtered = df.filter(col("year").isNotNull())
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞: —Å–∫–æ–ª—å–∫–æ —Å—Ç—Ä–æ–∫ –æ—Å—Ç–∞–ª–æ—Å—å –ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
    filtered_count = df_filtered.count()
    print(f"\nüìä –°—Ç—Ä–æ–∫ –ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ NULL: {filtered_count}")
    
    if filtered_count == 0:
        print("‚ùå ERROR: –í—Å–µ —Å—Ç—Ä–æ–∫–∏ –±—ã–ª–∏ –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω—ã!")
        return None
    
    # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Å—Ä–µ–¥–Ω–µ–≥–æ –≥–æ–¥–∞
    avg_year_result = df_filtered.select(avg("year")).collect()
    print(f"Debug: avg_year_result = {avg_year_result}")
    avg_year = avg_year_result[0][0] if avg_year_result and avg_year_result[0][0] is not None else None
    
    if avg_year is None:
        print("‚ùå ERROR: avg_year is None!")
        return None
        
    print(f"\nüìä –°—Ä–µ–¥–Ω–∏–π –≥–æ–¥ –ø–æ—Å—Ç—Ä–æ–π–∫–∏: {avg_year:.2f}")
    
    # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –º–µ–¥–∏–∞–Ω–Ω–æ–≥–æ –≥–æ–¥–∞
    median_year = df_filtered.stat.approxQuantile("year", [0.5], 0.01)[0]
    print(f"üìä –ú–µ–¥–∏–∞–Ω–Ω—ã–π –≥–æ–¥ –ø–æ—Å—Ç—Ä–æ–π–∫–∏: {median_year}")
    
    context['ti'].xcom_push(key='avg_year', value=avg_year)
    context['ti'].xcom_push(key='median_year', value=median_year)
    
    return {"avg_year": avg_year, "median_year": median_year}


def top_regions_and_cities(**context):
    """
    –ó–∞–¥–∞—á–∞ 5: –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–æ–ø-10 –æ–±–ª–∞—Å—Ç–µ–π –∏ –≥–æ—Ä–æ–¥–æ–≤ —Å –Ω–∞–∏–±–æ–ª—å—à–∏–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –æ–±—ä–µ–∫—Ç–æ–≤
    """
    print("=" * 80)
    print("–ó–ê–î–ê–ß–ê 5: –¢–æ–ø-10 —Ä–µ–≥–∏–æ–Ω–æ–≤ –∏ –≥–æ—Ä–æ–¥–æ–≤")
    print("=" * 80)
    
    spark = get_spark_session()
    df = read_parquet_data(spark)
    
    # –¢–æ–ø-10 —Ä–µ–≥–∏–æ–Ω–æ–≤
    print("\nüìä –¢–æ–ø-10 —Ä–µ–≥–∏–æ–Ω–æ–≤ –ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É –æ–±—ä–µ–∫—Ç–æ–≤:")
    top_regions = df.groupBy("region") \
        .agg(count("*").alias("count")) \
        .orderBy(col("count").desc()) \
        .limit(10)
    
    top_regions.show(truncate=False)
    
    # –¢–æ–ø-10 –≥–æ—Ä–æ–¥–æ–≤
    print("\nüìä –¢–æ–ø-10 –≥–æ—Ä–æ–¥–æ–≤ –ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É –æ–±—ä–µ–∫—Ç–æ–≤:")
    top_cities = df.groupBy("city") \
        .agg(count("*").alias("count")) \
        .orderBy(col("count").desc()) \
        .limit(10)
    
    top_cities.show(truncate=False)
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–æ–≤
    try:
        # –ì—Ä–∞—Ñ–∏–∫ –¥–ª—è —Ä–µ–≥–∏–æ–Ω–æ–≤
        regions_data = top_regions.collect()
        regions_names = [row['region'] for row in regions_data]
        regions_counts = [row['count'] for row in regions_data]
        
        plt.figure(figsize=(12, 6))
        plt.barh(regions_names, regions_counts)
        plt.xlabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–±—ä–µ–∫—Ç–æ–≤')
        plt.ylabel('–†–µ–≥–∏–æ–Ω')
        plt.title('–¢–æ–ø-10 —Ä–µ–≥–∏–æ–Ω–æ–≤ –ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É –æ–±—ä–µ–∫—Ç–æ–≤')
        plt.tight_layout()
        plt.savefig('/opt/airflow/data/top_regions.png', dpi=300, bbox_inches='tight')
        plt.close()
        print("\n‚úÖ –ì—Ä–∞—Ñ–∏–∫ —Ç–æ–ø-10 —Ä–µ–≥–∏–æ–Ω–æ–≤ —Å–æ—Ö—Ä–∞–Ω–µ–Ω: /opt/airflow/data/top_regions.png")
        
        # –ì—Ä–∞—Ñ–∏–∫ –¥–ª—è –≥–æ—Ä–æ–¥–æ–≤
        cities_data = top_cities.collect()
        cities_names = [row['city'] for row in cities_data]
        cities_counts = [row['count'] for row in cities_data]
        
        plt.figure(figsize=(12, 6))
        plt.barh(cities_names, cities_counts)
        plt.xlabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–±—ä–µ–∫—Ç–æ–≤')
        plt.ylabel('–ì–æ—Ä–æ–¥')
        plt.title('–¢–æ–ø-10 –≥–æ—Ä–æ–¥–æ–≤ –ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É –æ–±—ä–µ–∫—Ç–æ–≤')
        plt.tight_layout()
        plt.savefig('/opt/airflow/data/top_cities.png', dpi=300, bbox_inches='tight')
        plt.close()
        print("‚úÖ –ì—Ä–∞—Ñ–∏–∫ —Ç–æ–ø-10 –≥–æ—Ä–æ–¥–æ–≤ —Å–æ—Ö—Ä–∞–Ω–µ–Ω: /opt/airflow/data/top_cities.png")
        
    except Exception as e:
        print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ –≥—Ä–∞—Ñ–∏–∫–æ–≤: {e}")
    
    return True


def buildings_area_by_region(**context):
    """
    –ó–∞–¥–∞—á–∞ 6: –ù–∞–π—Ç–∏ –∑–¥–∞–Ω–∏—è —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –∏ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–π –ø–ª–æ—â–∞–¥—å—é –≤ —Ä–∞–º–∫–∞—Ö –∫–∞–∂–¥–æ–π –æ–±–ª–∞—Å—Ç–∏
    """
    print("=" * 80)
    print("–ó–ê–î–ê–ß–ê 6: –ó–¥–∞–Ω–∏—è —Å –º–∞–∫—Å/–º–∏–Ω –ø–ª–æ—â–∞–¥—å—é –≤ –∫–∞–∂–¥–æ–º —Ä–µ–≥–∏–æ–Ω–µ")
    print("=" * 80)
    
    spark = get_spark_session()
    df = read_parquet_data(spark)
    
    df_filtered = df.filter(col("area").isNotNull())
    
    # –ê–≥—Ä–µ–≥–∞—Ü–∏—è –ø–æ —Ä–µ–≥–∏–æ–Ω–∞–º
    area_stats = df_filtered.groupBy("region") \
        .agg(
            spark_max("area").alias("max_area"),
            spark_min("area").alias("min_area"),
            count("*").alias("count")
        ) \
        .orderBy(col("max_area").desc())
    
    print("\n–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–ª–æ—â–∞–¥–µ–π –ø–æ —Ä–µ–≥–∏–æ–Ω–∞–º (—Ç–æ–ø-20):")
    area_stats.show(20, truncate=False)
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–∞
    try:
        stats_data = area_stats.limit(15).collect()
        regions = [row['region'] for row in stats_data]
        max_areas = [row['max_area'] for row in stats_data]
        min_areas = [row['min_area'] for row in stats_data]
        
        x = range(len(regions))
        width = 0.35
        
        fig, ax = plt.subplots(figsize=(14, 8))
        ax.bar([i - width/2 for i in x], max_areas, width, label='–ú–∞–∫—Å. –ø–ª–æ—â–∞–¥—å', alpha=0.8)
        ax.bar([i + width/2 for i in x], min_areas, width, label='–ú–∏–Ω. –ø–ª–æ—â–∞–¥—å', alpha=0.8)
        
        ax.set_xlabel('–†–µ–≥–∏–æ–Ω')
        ax.set_ylabel('–ü–ª–æ—â–∞–¥—å (–∫–≤.–º)')
        ax.set_title('–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –∏ –º–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –ø–ª–æ—â–∞–¥—å –∑–¥–∞–Ω–∏–π –ø–æ —Ä–µ–≥–∏–æ–Ω–∞–º')
        ax.set_xticks(x)
        ax.set_xticklabels(regions, rotation=45, ha='right')
        ax.legend()
        
        plt.tight_layout()
        plt.savefig('/opt/airflow/data/area_by_region.png', dpi=300, bbox_inches='tight')
        plt.close()
        print("\n‚úÖ –ì—Ä–∞—Ñ–∏–∫ –ø–ª–æ—â–∞–¥–µ–π –ø–æ —Ä–µ–≥–∏–æ–Ω–∞–º —Å–æ—Ö—Ä–∞–Ω–µ–Ω: /opt/airflow/data/area_by_region.png")
        
    except Exception as e:
        print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ –≥—Ä–∞—Ñ–∏–∫–∞: {e}")
    
    return True


def buildings_by_decade(**context):
    """
    –ó–∞–¥–∞—á–∞ 7: –û–ø—Ä–µ–¥–µ–ª–∏—Ç—å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–¥–∞–Ω–∏–π –ø–æ –¥–µ—Å—è—Ç–∏–ª–µ—Ç–∏—è–º
    """
    print("=" * 80)
    print("–ó–ê–î–ê–ß–ê 7: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–¥–∞–Ω–∏–π –ø–æ –¥–µ—Å—è—Ç–∏–ª–µ—Ç–∏—è–º")
    print("=" * 80)
    
    spark = get_spark_session()
    df = read_parquet_data(spark)
    
    df_filtered = df.filter(col("year").isNotNull())
    
    # –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø–æ –¥–µ—Å—è—Ç–∏–ª–µ—Ç–∏—è–º
    df_decades = df_filtered \
        .withColumn("decade", (floor(col("year") / 10) * 10).cast(IntegerType())) \
        .groupBy("decade") \
        .agg(count("*").alias("count")) \
        .orderBy("decade")
    
    print("\n–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–¥–∞–Ω–∏–π –ø–æ –¥–µ—Å—è—Ç–∏–ª–µ—Ç–∏—è–º:")
    df_decades.show(50, truncate=False)
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–∞
    try:
        decades_data = df_decades.collect()
        decades = [row['decade'] for row in decades_data]
        counts = [row['count'] for row in decades_data]
        
        plt.figure(figsize=(14, 7))
        plt.bar(decades, counts, width=8, edgecolor='black', alpha=0.7)
        plt.xlabel('–î–µ—Å—è—Ç–∏–ª–µ—Ç–∏–µ')
        plt.ylabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–¥–∞–Ω–∏–π')
        plt.title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∑–¥–∞–Ω–∏–π –ø–æ –¥–µ—Å—è—Ç–∏–ª–µ—Ç–∏—è–º –ø–æ—Å—Ç—Ä–æ–π–∫–∏')
        plt.xticks(decades, [f"{d}s" for d in decades], rotation=45)
        plt.grid(axis='y', alpha=0.3)
        
        plt.tight_layout()
        plt.savefig('/opt/airflow/data/buildings_by_decade.png', dpi=300, bbox_inches='tight')
        plt.close()
        print("\n‚úÖ –ì—Ä–∞—Ñ–∏–∫ –∑–¥–∞–Ω–∏–π –ø–æ –¥–µ—Å—è—Ç–∏–ª–µ—Ç–∏—è–º —Å–æ—Ö—Ä–∞–Ω–µ–Ω: /opt/airflow/data/buildings_by_decade.png")
        
    except Exception as e:
        print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ –≥—Ä–∞—Ñ–∏–∫–∞: {e}")
    
    return True


def create_clickhouse_table(**context):
    """
    –ó–∞–¥–∞—á–∞ 8: –°–æ–∑–¥–∞—Ç—å —Å—Ö–µ–º—É —Ç–∞–±–ª–∏—Ü—ã –≤ ClickHouse
    """
    print("=" * 80)
    print("–ó–ê–î–ê–ß–ê 8: –°–æ–∑–¥–∞–Ω–∏–µ —Ç–∞–±–ª–∏—Ü—ã –≤ ClickHouse")
    print("=" * 80)
    
    try:
        # –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ ClickHouse
        client = clickhouse_connect.get_client(
            host=CH_HOST,
            port=CH_PORT,
            username=CH_USER,
            password=CH_PASSWORD
        )
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö
        print(f"\n–°–æ–∑–¥–∞–Ω–∏–µ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö {CH_DATABASE}...")
        client.command(f"CREATE DATABASE IF NOT EXISTS {CH_DATABASE}")
        print(f"‚úÖ –ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö {CH_DATABASE} —Å–æ–∑–¥–∞–Ω–∞")
        
        # –°–æ–∑–¥–∞–Ω–∏–µ —Ç–∞–±–ª–∏—Ü—ã
        print("\n–°–æ–∑–¥–∞–Ω–∏–µ —Ç–∞–±–ª–∏—Ü—ã houses...")
        create_table_query = f"""
        CREATE TABLE IF NOT EXISTS {CH_DATABASE}.houses
        (
            id Int32,
            latitude Float64,
            longitude Float64,
            year Nullable(Int32),
            area Nullable(Float64),
            floors Nullable(Int32),
            region Nullable(String),
            city Nullable(String),
            address Nullable(String),
            description Nullable(String)
        )
        ENGINE = MergeTree()
        ORDER BY id
        """
        
        client.command(create_table_query)
        print("‚úÖ –¢–∞–±–ª–∏—Ü–∞ houses —Å–æ–∑–¥–∞–Ω–∞ —É—Å–ø–µ—à–Ω–æ")
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ç–∞–±–ª–∏—Ü—ã
        result = client.command(f"SHOW TABLES FROM {CH_DATABASE}")
        print(f"\n–¢–∞–±–ª–∏—Ü—ã –≤ –±–∞–∑–µ {CH_DATABASE}: {result}")
        
        client.close()
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ —Ç–∞–±–ª–∏—Ü—ã: {e}")
        raise
    
    return True


def load_data_to_clickhouse(**context):
    """
    –ó–∞–¥–∞—á–∞ 10: –ó–∞–≥—Ä—É–∑–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏–∑ Parquet –≤ —Ç–∞–±–ª–∏—Ü—É –≤ ClickHouse
    """
    print("=" * 80)
    print("–ó–ê–î–ê–ß–ê 10: –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –≤ ClickHouse")
    print("=" * 80)
    
    spark = get_spark_session()
    
    # –ß–∏—Ç–∞–µ–º –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
    df = read_parquet_data(spark)
    
    print(f"\n–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–æ–∫ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏: {df.count()}")
    
    try:
        # –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ ClickHouse
        client = clickhouse_connect.get_client(
            host=CH_HOST,
            port=CH_PORT,
            username=CH_USER,
            password=CH_PASSWORD,
            database=CH_DATABASE
        )
        
        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º Spark DataFrame –≤ Pandas –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏
        print("\n–ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è DataFrame –≤ Pandas...")
        pandas_df = df.toPandas()
        
        print(f"–†–∞–∑–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö: {len(pandas_df)} —Å—Ç—Ä–æ–∫")
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –±–∞—Ç—á–∞–º–∏
        print("\n–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –≤ ClickHouse...")
        batch_size = 50000
        total_rows = len(pandas_df)
        
        for i in range(0, total_rows, batch_size):
            batch = pandas_df.iloc[i:i+batch_size]
            client.insert_df(
                'houses',
                batch
            )
            print(f"  –ó–∞–≥—Ä—É–∂–µ–Ω–æ {min(i+batch_size, total_rows)}/{total_rows} —Å—Ç—Ä–æ–∫")
        
        print("‚úÖ –î–∞–Ω–Ω—ã–µ —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –≤ ClickHouse")
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Å—Ç—Ä–æ–∫ –≤ —Ç–∞–±–ª–∏—Ü–µ
        count_result = client.query("SELECT count() as cnt FROM houses")
        db_count = count_result.result_rows[0][0]
        print(f"\nüìä –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–æ–∫ –≤ —Ç–∞–±–ª–∏—Ü–µ ClickHouse: {db_count}")
        
        client.close()
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –¥–∞–Ω–Ω—ã—Ö: {e}")
        raise
    
    return True


def query_top_houses(**context):
    """
    –ó–∞–¥–∞—á–∞ 11: –í—ã–ø–æ–ª–Ω–∏—Ç—å SQL —Å–∫—Ä–∏–ø—Ç, –∫–æ—Ç–æ—Ä—ã–π –≤—ã–≤–µ–¥–µ—Ç —Ç–æ–ø 25 –¥–æ–º–æ–≤ —Å –ø–ª–æ—â–∞–¥—å—é –±–æ–ª—å—à–µ 60 –∫–≤.–º
    """
    print("=" * 80)
    print("–ó–ê–î–ê–ß–ê 11: –ó–∞–ø—Ä–æ—Å —Ç–æ–ø-25 –¥–æ–º–æ–≤ —Å –ø–ª–æ—â–∞–¥—å—é > 60 –∫–≤.–º")
    print("=" * 80)
    
    try:
        # –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ ClickHouse
        client = clickhouse_connect.get_client(
            host=CH_HOST,
            port=CH_PORT,
            username=CH_USER,
            password=CH_PASSWORD,
            database=CH_DATABASE
        )
        
        # SQL –∑–∞–ø—Ä–æ—Å
        query = """
        SELECT 
            id,
            region,
            city,
            address,
            area,
            year,
            floors
        FROM houses
        WHERE area > 60
        ORDER BY area DESC
        LIMIT 25
        """
        
        print("\n–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ SQL –∑–∞–ø—Ä–æ—Å–∞...")
        result = client.query(query)
        
        print("\nüìä –¢–æ–ø-25 –¥–æ–º–æ–≤ —Å –ø–ª–æ—â–∞–¥—å—é –±–æ–ª—å—à–µ 60 –∫–≤.–º:")
        print("=" * 150)
        print(f"{'ID':<10} {'–†–µ–≥–∏–æ–Ω':<30} {'–ì–æ—Ä–æ–¥':<25} {'–ü–ª–æ—â–∞–¥—å':<12} {'–ì–æ–¥':<10} {'–≠—Ç–∞–∂–µ–π':<10}")
        print("=" * 150)
        
        for row in result.result_rows:
            id_val, region, city, address, area, year, floors = row
            print(f"{id_val:<10} {region[:28]:<30} {city[:23]:<25} {area:<12.2f} {year if year else 'N/A':<10} {floors if floors else 'N/A':<10}")
        
        print("=" * 150)
        print(f"\n‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(result.result_rows)} –∑–∞–ø–∏—Å–µ–π")
        
        client.close()
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏ –∑–∞–ø—Ä–æ—Å–∞: {e}")
        raise
    
    return True


# –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ DAG
default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'start_date': datetime(2024, 1, 1),
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

dag = DAG(
    'russian_houses_analysis',
    default_args=default_args,
    description='–ê–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö –æ —Ä–æ—Å—Å–∏–π—Å–∫–∏—Ö –¥–æ–º–∞—Ö —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º PySpark –∏ ClickHouse',
    schedule_interval=None,
    catchup=False,
    tags=['pyspark', 'clickhouse', 'data-analysis'],
)

# –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∑–∞–¥–∞—á
task_0_prepare = PythonOperator(
    task_id='prepare_data_to_parquet',
    python_callable=prepare_data_to_parquet,
    dag=dag,
)

task_1_load = PythonOperator(
    task_id='load_csv_to_spark',
    python_callable=load_csv_to_spark,
    dag=dag,
)

task_2_validate = PythonOperator(
    task_id='validate_data',
    python_callable=validate_data,
    dag=dag,
)

task_3_transform = PythonOperator(
    task_id='transform_data',
    python_callable=transform_data,
    dag=dag,
)

task_4_year_stats = PythonOperator(
    task_id='calculate_year_statistics',
    python_callable=calculate_year_statistics,
    dag=dag,
)

task_5_top_regions = PythonOperator(
    task_id='top_regions_and_cities',
    python_callable=top_regions_and_cities,
    dag=dag,
)

task_6_area_stats = PythonOperator(
    task_id='buildings_area_by_region',
    python_callable=buildings_area_by_region,
    dag=dag,
)

task_7_decades = PythonOperator(
    task_id='buildings_by_decade',
    python_callable=buildings_by_decade,
    dag=dag,
)

task_8_create_table = PythonOperator(
    task_id='create_clickhouse_table',
    python_callable=create_clickhouse_table,
    dag=dag,
)

task_10_load_to_ch = PythonOperator(
    task_id='load_data_to_clickhouse',
    python_callable=load_data_to_clickhouse,
    dag=dag,
)

task_11_query = PythonOperator(
    task_id='query_top_houses',
    python_callable=query_top_houses,
    dag=dag,
)

task_cleanup = PythonOperator(
    task_id='cleanup_spark',
    python_callable=stop_spark_session,
    dag=dag,
)

# –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π –º–µ–∂–¥—É –∑–∞–¥–∞—á–∞–º–∏
task_0_prepare >> task_1_load >> task_2_validate >> task_3_transform
# –ó–∞–¥–∞—á–∏ 4-7 –≤—ã–ø–æ–ª–Ω—è—é—Ç—Å—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ, —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –Ω–µ—Ö–≤–∞—Ç–∫–∏ –ø–∞–º—è—Ç–∏
task_3_transform >> task_4_year_stats >> task_5_top_regions >> task_6_area_stats >> task_7_decades
task_7_decades >> task_8_create_table
task_8_create_table >> task_10_load_to_ch >> task_11_query >> task_cleanup
