Павел Гапченко, [26.02.2026 22:52]
Первый момент: при каждой таске вы заного читаете исходный csv. Это не гарантирует, что в дальнейшем вы работаете с очищенными данными. То есть чистите данные вроде, но потом опять читаете исходник и работаете с ним. Плюс каждый раз читает 300 МБ файл, это затратно по ресурсам. Гораздо лучше сначала прочитать исходник, очистить, привести типы и сохранить в паркет, а потом работать с ним

Павел Гапченко, [26.02.2026 22:53]
второй момент

Павел Гапченко, [26.02.2026 22:53]
когда пайплайн выполнился, в итоге я получил вот

Павел Гапченко, [26.02.2026 22:53]
пустая таблица

Павел Гапченко, [26.02.2026 22:55]
тут либо преобразовать в другую кодировку, UTF-8 например или добавить параметры при чтении, мне помогло такое решение:

def read_csv_with_bom_fix(spark):
    """Чтение CSV с правильной кодировкой (UTF-16 LE)"""
    # CSV файл имеет UTF-16 LE BOM (0xFF 0xFE), поэтому читаем с правильной кодировкой
    df = spark.read.csv(
        CSV_FILE_PATH,
        header=True,
        sep=',',
        #inferSchema=True,
        encoding='UTF-16LE',
        quote='"',
        escape='"',
        multiLine=True,
        mode="PERMISSIVE",
        columnNameOfCorruptRecord="_corrupt_record"
    )
    
    print(f"✅ CSV прочитан. Колонки: {df.columns}")
    
    return df

Павел Гапченко, [26.02.2026 22:55]
и в итоге вот

Павел Гапченко, [26.02.2026 22:57]
и еще небольшой момент. У вас в репо написано так:
# Файл должен находиться здесь:
# nova_data_house_analysis_task/russian_houses.csv
Но по факту исходник должен лежать в папке data. Из за этого ошибки сыпятся. В репо это не указано и появляется путаница.

Erjan K, [26.02.2026 22:58]
блин мда

Павел Гапченко, [26.02.2026 22:59]
Это бы поправить и я поставлю зачет. Вторую работу утром проверю, потому что много времени ушло на эту работу

Павел Гапченко, [26.02.2026 23:01]
тут либо предусмотреть авто копирование файла, либо написать, что нужно его туда положить))

Erjan K, [26.02.2026 23:09]
понял делаю